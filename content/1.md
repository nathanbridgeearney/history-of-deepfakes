---
stepLabel: "Dubbing and 3D Models"

implicationTitle: "Informed Consent in Deepfake Research"
implicationDescription: |
  The origins of modern deepfakes can be found in the 1990’s, with hopeful goals of reducing the manual labour involved with special effects and image post-processing. However, even in these early years a piece of the ethical monster that deepfakes later become is present. The earliest related paper I found [1] uses an extremely famous politician to show that they can generate fake video of them talking. Using politicians, John F. Kennedy in this case, remains a consistent trend for many of the papers that will be referenced as you move through this timeline. This is often attributed to the higher than usual quality of media that contain politicians and its abundance. The second paper [2] features another group that is often included in these papers, actors and actresses. Specifically, Tom Hanks is featured, however unlike voluntary participants his name is not mentioned, he is not credited nor is there any indication that he consented to his image being used and published. 

  I believe that the lack of consent is the single greatest issue surrounding deepfakes and I will argue that researchers have perpetrated this within their articles. These early papers set a precedent for using public figures within this field of research and have ultimately led us to politicians and public figures turning this technology onto each other for their own gain.

applicationTitle: "[1] Video Rewrite: Driving Visual Speech with Audio, [2] A Morphable Model For The Synthesis Of 3D Faces"
applicationDescription: |
  [1] - The authors have created a method to generate a video of a person saying anything they want via audio input. It uses computer-vision to track specific points of the mouth in the original video and morphs these points to match the target audio. This was the first facial-animation system that was automated and was able to generate a video of John F. Kennedy saying, “Read my lips” using only two minutes of footage. For each person it rewrites it needs 26 labelled images, the only human input in the process. 

  [2] - This paper introduces a method of creating a textured 3D morphable model of a face. The model can be created through a UI or through images, and is not limited to photographs of real people, it can work just as well when the photograph is of a painting. By then morphing the generated model it allows for new views of a face from different angles and lighting conditions along with other attributes such as age, weight, gender, face fullness and expressions generated in a photorealistic way for the time.

  Both technologies have applications within the film industry, specifically for dubbing and special effects. The first was a good start to skip small reshoots, changing a single line or dubbing to a different language. The second allows for more advanced changes, adjusting the lighting, adding a hat or adjusting the facial structure of an actor.
---
